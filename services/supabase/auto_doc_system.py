#!/usr/bin/env python3
"""
Self-Documenting API System
Automatically keeps documentation in sync with your actual database
"""

import json
import schedule
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

from services.supabase.api import SupabaseAPI
from services.supabase.openapi_generator import SupabaseOpenAPIGenerator


class SelfDocumentingAPI:
    """
    This system automatically:
    1. Discovers your database schema
    2. Generates up-to-date documentation
    3. Creates interactive API explorers
    4. Detects schema changes
    5. Updates client SDKs
    """
    
    def __init__(self):
        self.projects = ['project1', 'project2', 'project3']
        self.docs_dir = Path(__file__).parent / 'auto_docs'
        self.docs_dir.mkdir(exist_ok=True)
        
        # Track schema versions
        self.schema_cache_file = self.docs_dir / 'schema_cache.json'
        self.schema_cache = self.load_schema_cache()
    
    def load_schema_cache(self) -> Dict:
        """Load previously discovered schemas"""
        if self.schema_cache_file.exists():
            with open(self.schema_cache_file, 'r') as f:
                return json.load(f)
        return {}
    
    def discover_and_document(self, project: str) -> Dict:
        """
        Automatically discover database structure and generate docs
        """
        print(f"\nğŸ” Discovering {project} database...")
        
        api = SupabaseAPI(project)
        discovered = {
            'timestamp': datetime.now().isoformat(),
            'project': project,
            'tables': {},
            'changes': []
        }
        
        # Discover all tables
        try:
            # Get tables using exploration
            tables_info = []
            
            # Try to get actual tables from API
            sample_tables = {
                'project1': ['brands', 'leads', 'scraping_results'],
                'project2': ['customers', 'orders', 'products'],
                'project3': ['scrape_guide', 'scrape_results', 'scrape_queue']
            }
            
            for table in sample_tables.get(project, []):
                try:
                    # Get schema for each table
                    schema = api.get_schema(table)
                    count = api.count(table)
                    
                    # Get sample data for better type inference
                    sample = api.query(table, limit=1)
                    
                    table_info = {
                        'name': table,
                        'row_count': count,
                        'columns': {},
                        'sample_data': sample[0] if sample else {}
                    }
                    
                    # Analyze each column
                    for col_info in schema:
                        column = col_info['column']
                        table_info['columns'][column] = {
                            'type': col_info['type'],
                            'inferred_type': self.infer_detailed_type(column, col_info.get('sample')),
                            'nullable': self.check_nullable(api, table, column),
                            'unique': self.check_unique(api, table, column),
                            'sample': str(col_info.get('sample', ''))[:100]
                        }
                    
                    discovered['tables'][table] = table_info
                    
                    # Detect changes
                    if project in self.schema_cache:
                        old_table = self.schema_cache[project].get('tables', {}).get(table, {})
                        changes = self.detect_changes(old_table, table_info)
                        if changes:
                            discovered['changes'].extend(changes)
                    
                except Exception as e:
                    print(f"  âš ï¸ Could not analyze {table}: {e}")
            
        except Exception as e:
            print(f"  âŒ Error discovering tables: {e}")
        
        # Save to cache
        self.schema_cache[project] = discovered
        with open(self.schema_cache_file, 'w') as f:
            json.dump(self.schema_cache, f, indent=2, default=str)
        
        return discovered
    
    def infer_detailed_type(self, column_name: str, sample_value: Any) -> str:
        """
        Infer detailed type from column name and sample data
        """
        # Email detection
        if 'email' in column_name.lower():
            return 'email'
        
        # UUID detection
        if 'id' in column_name.lower() and sample_value:
            if len(str(sample_value)) == 36 and '-' in str(sample_value):
                return 'uuid'
        
        # Date/time detection
        if any(dt in column_name.lower() for dt in ['created', 'updated', 'date', 'time']):
            return 'datetime'
        
        # URL detection
        if 'url' in column_name.lower() or 'link' in column_name.lower():
            return 'url'
        
        # Phone detection
        if 'phone' in column_name.lower() or 'mobile' in column_name.lower():
            return 'phone'
        
        # Status/enum detection
        if 'status' in column_name.lower() or 'type' in column_name.lower():
            return 'enum'
        
        # Numeric types
        if any(num in column_name.lower() for num in ['price', 'amount', 'total', 'cost']):
            return 'currency'
        if any(num in column_name.lower() for num in ['score', 'rating', 'rank']):
            return 'number'
        if any(num in column_name.lower() for num in ['count', 'quantity', 'inventory']):
            return 'integer'
        
        # Boolean detection
        if any(bool_word in column_name.lower() for bool_word in ['is_', 'has_', 'active', 'verified']):
            return 'boolean'
        
        # JSON detection
        if 'json' in column_name.lower() or 'data' in column_name.lower():
            return 'json'
        
        return 'string'
    
    def check_nullable(self, api: SupabaseAPI, table: str, column: str) -> bool:
        """Check if column allows nulls (simplified)"""
        # Would need actual database introspection
        # For now, assume IDs are not nullable
        return 'id' not in column.lower()
    
    def check_unique(self, api: SupabaseAPI, table: str, column: str) -> bool:
        """Check if column has unique constraint (simplified)"""
        # Would need actual database introspection
        # For now, assume IDs and emails are unique
        return 'id' in column.lower() or 'email' in column.lower()
    
    def detect_changes(self, old_table: Dict, new_table: Dict) -> List[str]:
        """Detect schema changes between versions"""
        changes = []
        
        if not old_table:
            return [f"ğŸ“ New table discovered: {new_table['name']}"]
        
        old_columns = set(old_table.get('columns', {}).keys())
        new_columns = set(new_table.get('columns', {}).keys())
        
        # New columns
        added = new_columns - old_columns
        for col in added:
            changes.append(f"â• New column: {new_table['name']}.{col}")
        
        # Removed columns
        removed = old_columns - new_columns
        for col in removed:
            changes.append(f"â– Removed column: {new_table['name']}.{col}")
        
        # Type changes
        for col in old_columns & new_columns:
            old_type = old_table['columns'][col].get('inferred_type')
            new_type = new_table['columns'][col].get('inferred_type')
            if old_type != new_type:
                changes.append(f"ğŸ”„ Type changed: {new_table['name']}.{col} from {old_type} to {new_type}")
        
        # Row count changes (significant)
        old_count = old_table.get('row_count', 0)
        new_count = new_table.get('row_count', 0)
        if abs(new_count - old_count) > 1000:
            changes.append(f"ğŸ“Š Significant data change: {new_table['name']} ({old_count} â†’ {new_count} rows)")
        
        return changes
    
    def generate_documentation(self) -> None:
        """
        Generate all documentation formats
        """
        print("\nğŸ“š Generating documentation...")
        
        for project in self.projects:
            # 1. Discover schema
            discovery = self.discover_and_document(project)
            
            # 2. Generate OpenAPI spec
            generator = SupabaseOpenAPIGenerator(project)
            spec_file = generator.save_spec('yaml')
            print(f"  âœ“ OpenAPI spec: {spec_file}")
            
            # 3. Generate Markdown documentation
            self.generate_markdown_docs(project, discovery)
            
            # 4. Generate TypeScript types
            ts_file = self.generate_typescript_types(project, discovery)
            print(f"  âœ“ TypeScript types: {ts_file}")
            
            # 5. Generate validation schemas
            validation_file = self.generate_validation_schemas(project, discovery)
            print(f"  âœ“ Validation schemas: {validation_file}")
            
            # 6. Report changes
            if discovery['changes']:
                print(f"\n  ğŸ”” Schema changes detected in {project}:")
                for change in discovery['changes']:
                    print(f"    {change}")
    
    def generate_markdown_docs(self, project: str, discovery: Dict) -> Path:
        """Generate human-readable Markdown documentation"""
        doc_file = self.docs_dir / f"{project}_api_docs.md"
        
        content = f"""# {project.title()} API Documentation

*Auto-generated: {discovery['timestamp']}*

## Database Overview

**Total Tables**: {len(discovery['tables'])}  
**Total Rows**: {sum(t.get('row_count', 0) for t in discovery['tables'].values())}

## Tables

"""
        
        for table_name, table_info in discovery['tables'].items():
            content += f"""### {table_name}

**Description**: {self.get_table_description(project, table_name)}  
**Row Count**: {table_info['row_count']:,}

#### Columns

| Column | Type | Nullable | Unique | Description |
|--------|------|----------|--------|-------------|
"""
            
            for col_name, col_info in table_info['columns'].items():
                content += (f"| `{col_name}` "
                          f"| {col_info['inferred_type']} "
                          f"| {'Yes' if col_info['nullable'] else 'No'} "
                          f"| {'Yes' if col_info['unique'] else 'No'} "
                          f"| {self.get_column_description(col_name)} |\n")
            
            content += f"""

#### Sample Query

```python
from api_toolkit.services.supabase.api import SupabaseAPI

api = SupabaseAPI('{project}')
results = api.query('{table_name}', limit=10)
```

#### Filter Examples

```python
# Get active records
api.query('{table_name}', filters={{'status': 'eq.active'}})

# Get recent records
api.query('{table_name}', order='-created_at', limit=20)
```

---

"""
        
        with open(doc_file, 'w') as f:
            f.write(content)
        
        return doc_file
    
    def generate_typescript_types(self, project: str, discovery: Dict) -> Path:
        """Generate TypeScript type definitions"""
        ts_file = self.docs_dir / f"{project}_types.ts"
        
        content = f"""// Auto-generated TypeScript types for {project}
// Generated: {discovery['timestamp']}

"""
        
        for table_name, table_info in discovery['tables'].items():
            # Convert table name to PascalCase
            interface_name = ''.join(word.capitalize() for word in table_name.split('_'))
            
            content += f"export interface {interface_name} {{\n"
            
            for col_name, col_info in table_info['columns'].items():
                ts_type = self.inferred_to_typescript(col_info['inferred_type'])
                nullable = '?' if col_info['nullable'] else ''
                content += f"  {col_name}{nullable}: {ts_type};\n"
            
            content += "}\n\n"
        
        with open(ts_file, 'w') as f:
            f.write(content)
        
        return ts_file
    
    def generate_validation_schemas(self, project: str, discovery: Dict) -> Path:
        """Generate JSON Schema for validation"""
        schema_file = self.docs_dir / f"{project}_validation.json"
        
        schemas = {}
        
        for table_name, table_info in discovery['tables'].items():
            table_schema = {
                "type": "object",
                "properties": {},
                "required": []
            }
            
            for col_name, col_info in table_info['columns'].items():
                prop_schema = self.inferred_to_json_schema(col_info['inferred_type'])
                table_schema["properties"][col_name] = prop_schema
                
                if not col_info['nullable'] and col_name != 'id':
                    table_schema["required"].append(col_name)
            
            schemas[table_name] = table_schema
        
        with open(schema_file, 'w') as f:
            json.dump(schemas, f, indent=2)
        
        return schema_file
    
    def get_table_description(self, project: str, table: str) -> str:
        """Get description for a table"""
        descriptions = {
            'brands': 'Company and brand information',
            'leads': 'Potential customer leads with scoring',
            'scraping_results': 'Web scraping results and extracted data',
            'customers': 'Customer profiles and contact information',
            'orders': 'Customer orders and transactions',
            'products': 'Product catalog and inventory',
            'scrape_guide': 'Project 3 configuration and targets',
            'scrape_results': 'Results from scraping operations',
            'scrape_queue': 'Pending scraping jobs'
        }
        return descriptions.get(table, 'Data table')
    
    def get_column_description(self, column: str) -> str:
        """Get description for a column based on name"""
        descriptions = {
            'id': 'Unique identifier',
            'email': 'Email address',
            'created_at': 'Record creation timestamp',
            'updated_at': 'Last update timestamp',
            'status': 'Current status',
            'score': 'Numerical score or rating',
            'name': 'Name or title',
            'url': 'Web URL',
            'data': 'JSON data payload',
            'priority': 'Priority level',
            'active': 'Whether record is active'
        }
        
        for key, desc in descriptions.items():
            if key in column.lower():
                return desc
        
        return 'Data field'
    
    def inferred_to_typescript(self, inferred_type: str) -> str:
        """Convert inferred type to TypeScript type"""
        type_map = {
            'string': 'string',
            'email': 'string',
            'url': 'string',
            'phone': 'string',
            'uuid': 'string',
            'datetime': 'Date | string',
            'integer': 'number',
            'number': 'number',
            'currency': 'number',
            'boolean': 'boolean',
            'json': 'any',
            'enum': 'string'
        }
        return type_map.get(inferred_type, 'any')
    
    def inferred_to_json_schema(self, inferred_type: str) -> Dict:
        """Convert inferred type to JSON Schema"""
        schema_map = {
            'email': {"type": "string", "format": "email"},
            'url': {"type": "string", "format": "uri"},
            'uuid': {"type": "string", "format": "uuid"},
            'datetime': {"type": "string", "format": "date-time"},
            'phone': {"type": "string", "pattern": "^[+]?[0-9]{10,15}$"},
            'integer': {"type": "integer"},
            'number': {"type": "number"},
            'currency': {"type": "number", "minimum": 0},
            'boolean': {"type": "boolean"},
            'json': {"type": "object"},
            'enum': {"type": "string"},
            'string': {"type": "string"}
        }
        return schema_map.get(inferred_type, {"type": "string"})
    
    def watch_for_changes(self):
        """
        Continuously watch for schema changes
        """
        print("ğŸ‘ï¸ Watching for schema changes...")
        
        # Schedule checks every hour
        schedule.every(1).hours.do(self.check_and_update)
        
        # Initial generation
        self.generate_documentation()
        
        while True:
            schedule.run_pending()
            time.sleep(60)
    
    def check_and_update(self):
        """Check for changes and regenerate if needed"""
        print(f"\nğŸ”„ Checking for updates at {datetime.now()}")
        
        changes_detected = False
        
        for project in self.projects:
            old_discovery = self.schema_cache.get(project, {})
            new_discovery = self.discover_and_document(project)
            
            if new_discovery['changes']:
                changes_detected = True
                print(f"  ğŸ“ Changes in {project}: {len(new_discovery['changes'])} changes")
        
        if changes_detected:
            print("  ğŸ”¨ Regenerating documentation...")
            self.generate_documentation()
            print("  âœ… Documentation updated!")
        else:
            print("  âœ… No changes detected")
    
    def generate_api_index(self):
        """Generate a main index page for all APIs"""
        index_file = self.docs_dir / 'index.html'
        
        content = f"""<!DOCTYPE html>
<html>
<head>
    <title>API Documentation Portal</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; }}
        .container {{ max-width: 1200px; margin: 0 auto; padding: 20px; }}
        .project {{ border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin: 20px 0; }}
        .stats {{ display: flex; gap: 20px; margin: 10px 0; }}
        .stat {{ background: #f5f5f5; padding: 10px; border-radius: 4px; }}
        .links {{ margin-top: 15px; }}
        a {{ color: #0366d6; text-decoration: none; margin-right: 15px; }}
        a:hover {{ text-decoration: underline; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ Self-Documenting API Portal</h1>
        <p>Auto-generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        
"""
        
        for project in self.projects:
            if project in self.schema_cache:
                discovery = self.schema_cache[project]
                table_count = len(discovery.get('tables', {}))
                total_rows = sum(t.get('row_count', 0) for t in discovery.get('tables', {}).values())
                
                content += f"""
        <div class="project">
            <h2>{project.title()} API</h2>
            <div class="stats">
                <div class="stat">ğŸ“Š Tables: {table_count}</div>
                <div class="stat">ğŸ“ Total Rows: {total_rows:,}</div>
                <div class="stat">ğŸ• Updated: {discovery.get('timestamp', 'Unknown')}</div>
            </div>
            <div class="links">
                <a href="{project}_api_docs.md">ğŸ“– Documentation</a>
                <a href="{project}_types.ts">ğŸ“¦ TypeScript Types</a>
                <a href="../openapi/{project}_api.yaml">ğŸ“‹ OpenAPI Spec</a>
                <a href="{project}_validation.json">âœ… Validation Schema</a>
            </div>
        </div>
"""
        
        content += """
    </div>
</body>
</html>"""
        
        with open(index_file, 'w') as f:
            f.write(content)
        
        print(f"  âœ“ Generated API index: {index_file}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "watch":
        # Continuous monitoring mode
        system = SelfDocumentingAPI()
        system.watch_for_changes()
    else:
        # One-time generation
        system = SelfDocumentingAPI()
        system.generate_documentation()
        system.generate_api_index()
        
        print("\nâœ¨ Self-documentation complete!")
        print("\nGenerated files:")
        for file in system.docs_dir.glob("*"):
            print(f"  - {file.name}")
        
        print("\nTo continuously watch for changes:")
        print("  python auto_doc_system.py watch")